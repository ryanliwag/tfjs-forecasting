{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn\n",
    "# multivariate output stacked lstm example\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# split a univariate sequence into samples\\ndef split_sequence(sequence, n_steps_in, n_steps_out):\\n    X, y = list(), list()\\n    for i in range(len(sequence)):\\n        # find the end of this pattern\\n        end_ix = i + n_steps_in\\n        out_end_ix = end_ix + n_steps_out\\n        # check if we are beyond the sequence\\n        if out_end_ix > len(sequence):\\n            break\\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\\n        X.append(seq_x)\\n        y.append(seq_y)\\n    \\n    return np.array(X).reshape((len(X), n_steps_in, 1)), np.array(y)\\n \\n# define input sequence\\n#random.shuffle(dataset)\\n# choose a number of time steps\\nn_steps_in, n_steps_out = 10, 1\\n# split into samples\\nX, y = split_sequence(dataset, n_steps_in, n_steps_out)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"daily_coinbase.csv\")\n",
    "\n",
    "dataset=df[[\"coinbase\"]].fillna(method=\"ffill\")\n",
    "scl = MinMaxScaler(feature_range = (0, 1))\n",
    "#Scale the data\n",
    "dataset = scl.fit_transform(dataset)\n",
    "dataset = dataset.squeeze().tolist()\n",
    "\n",
    "n_steps_in = 14\n",
    "n_steps_out = 1\n",
    "def processData(data,lb):\n",
    "    X,Y = [],[]\n",
    "    for i in range(len(data)-lb-1):\n",
    "        X.append(data[i:(i+lb)])\n",
    "        Y.append(data[(i+lb)])\n",
    "    return np.array(X).reshape((len(X), n_steps_in, 1)),np.array(Y)\n",
    "\n",
    "X,y = processData(dataset,n_steps_in)\n",
    "\n",
    "'''\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the sequence\n",
    "        if out_end_ix > len(sequence):\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    \n",
    "    return np.array(X).reshape((len(X), n_steps_in, 1)), np.array(y)\n",
    " \n",
    "# define input sequence\n",
    "#random.shuffle(dataset)\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 10, 1\n",
    "# split into samples\n",
    "X, y = split_sequence(dataset, n_steps_in, n_steps_out)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09747555883591161,\n",
       " 0.12270944482262003,\n",
       " 0.16845289851208126,\n",
       " 0.17033520730930615,\n",
       " 0.18950539606793287,\n",
       " 0.14428938235403574,\n",
       " 0.2908494207588483,\n",
       " 0.17533418899353317,\n",
       " 0.0857788297094221,\n",
       " 0.0673483880008828,\n",
       " 0.05278916142740387,\n",
       " 0.15020417123317095,\n",
       " 0.09379026384699515,\n",
       " 0.07022501731369249,\n",
       " 0.13007645469855256,\n",
       " 0.0678046483609406,\n",
       " 0.04932989454851367,\n",
       " 0.08091135080127002,\n",
       " 0.07144391879772344,\n",
       " 0.05790702669065655,\n",
       " 0.047024958182076464,\n",
       " 0.05141858737157166,\n",
       " 0.16796992234682045,\n",
       " 0.05669977934400843,\n",
       " 0.08479240777774777,\n",
       " 0.12613150176345755,\n",
       " 0.09928981213161017,\n",
       " 0.06805931390305162,\n",
       " 0.11840930830015575,\n",
       " 0.14767054267036586,\n",
       " 0.1828544290500953,\n",
       " 0.16411065197715732,\n",
       " 0.26642261855686666,\n",
       " 0.1412281867785196,\n",
       " 0.15997198644336266,\n",
       " 0.1618022001496307,\n",
       " 0.07361287959380422,\n",
       " 0.053011444924680164,\n",
       " 0.09581027704813605,\n",
       " 0.17278208650191026,\n",
       " 0.08820444191540175,\n",
       " 0.0708213083235068,\n",
       " 0.0783575403047582,\n",
       " 0.03422098362768088,\n",
       " 0.041376417963950536,\n",
       " 0.07094962867989202,\n",
       " 0.10947695370144092,\n",
       " 0.07538714841954033,\n",
       " 0.10365459741877889,\n",
       " 0.13138382663501316,\n",
       " 0.15083068113945894,\n",
       " 0.08102109660203768,\n",
       " 0.17004692270402094,\n",
       " 0.133285350836359,\n",
       " 0.13289638863236003,\n",
       " 0.06104992357661803,\n",
       " 0.1733525566628371,\n",
       " 0.06708561156862528,\n",
       " 0.06994749778194051,\n",
       " 0.06166013166850912,\n",
       " 0.09335990062487923,\n",
       " 0.18690715123171223,\n",
       " 0.30177460729032674,\n",
       " 0.45948178859346833,\n",
       " 0.15255018279018512,\n",
       " 0.08677240397530223,\n",
       " 0.13988248227556127,\n",
       " 0.10244598851337083,\n",
       " 0.06716546655930808,\n",
       " 0.11589305163621694,\n",
       " 0.095602578413927,\n",
       " 0.041233625168305615,\n",
       " 0.02308224837293598,\n",
       " 0.06421808696218875,\n",
       " 0.03731735849797657,\n",
       " 0.09217410440735965,\n",
       " 0.05554477460323457,\n",
       " 0.07378634762799818,\n",
       " 0.03786445888967164,\n",
       " 0.028658548223213585,\n",
       " 0.06318607390449588,\n",
       " 0.07745809934846407,\n",
       " 0.04101662188185471,\n",
       " 0.04004764198678666,\n",
       " 0.03832821972650347,\n",
       " 0.016914483555299162,\n",
       " 0.046312449963368495,\n",
       " 0.11238819288026505,\n",
       " 0.1026823898491254,\n",
       " 0.04813274562772548,\n",
       " 0.14747408569658169,\n",
       " 0.21176351551845493,\n",
       " 0.06151817289132531,\n",
       " 0.08840680727425934,\n",
       " 0.06385347999702454,\n",
       " 0.062413507487262355,\n",
       " 0.1440910851812244,\n",
       " 0.061059386065143835,\n",
       " 0.12513909962881112,\n",
       " 0.11154895980039553,\n",
       " 0.08074483380280251,\n",
       " 0.12930487753814865,\n",
       " 0.1430324277120958,\n",
       " 0.10570365094990211,\n",
       " 0.08604027172993367,\n",
       " 0.06496773922830706,\n",
       " 0.028896891659251314,\n",
       " 0.0862394155045999,\n",
       " 0.05828992957772069,\n",
       " 0.1176098935076613,\n",
       " 0.17359722340648207,\n",
       " 0.3028329671605395,\n",
       " 0.19628574753090733,\n",
       " 0.10112323346470162,\n",
       " 0.12129003363122498,\n",
       " 0.1948137436518243,\n",
       " 0.11081421966605123,\n",
       " 0.28314164096271643,\n",
       " 0.14749200654644293,\n",
       " 0.2824781276220506,\n",
       " 0.21921052522979345,\n",
       " 0.46150653128429364,\n",
       " 0.3274022163940571,\n",
       " 0.1593054103914485,\n",
       " 0.2425676041600513,\n",
       " 0.2233558778787746,\n",
       " 0.19354244501939333,\n",
       " 0.10626447567379546,\n",
       " 0.10393545121554909,\n",
       " 0.12704912241338406,\n",
       " 0.23246188768099518,\n",
       " 0.10729557781486415,\n",
       " 0.09281240610778543,\n",
       " 0.10854801975944428,\n",
       " 0.12884763592071544,\n",
       " 0.24576141000603813,\n",
       " 0.21950949695044272,\n",
       " 0.15794147888245572,\n",
       " 0.6388363697723187,\n",
       " 0.3741204446209984,\n",
       " 0.2844042195154855,\n",
       " 0.1669281771805965,\n",
       " 0.2749345522089231,\n",
       " 0.16699379939090028,\n",
       " 0.14666444971017786,\n",
       " 0.4689806166101586,\n",
       " 0.7632295289742846,\n",
       " 0.6161884840095518,\n",
       " 0.37920261110971276,\n",
       " 0.4713457127394575,\n",
       " 0.3517434490371671,\n",
       " 0.34166366870900194,\n",
       " 0.2809152222590406,\n",
       " 0.13681111253133674,\n",
       " 0.24593726434096314,\n",
       " 0.2156345313466995,\n",
       " 0.18390425146152772,\n",
       " 0.22359402178521037,\n",
       " 0.2827495164088682,\n",
       " 0.44770225247732726,\n",
       " 0.265611997613628,\n",
       " 0.9999999999999999,\n",
       " 0.2867688361566505,\n",
       " 0.22579460381795888,\n",
       " 0.1268604504207943,\n",
       " 0.17565273998393924,\n",
       " 0.2009994799203219,\n",
       " 0.23090992038472707,\n",
       " 0.1485015044740997,\n",
       " 0.24191477180321724,\n",
       " 0.153203187862557,\n",
       " 0.0958335228659627,\n",
       " 0.2284193016029034,\n",
       " 0.15417489846492627,\n",
       " 0.170019693558449,\n",
       " 0.20044866640910974,\n",
       " 0.11305302916686857,\n",
       " 0.0983513257840426,\n",
       " 0.20785476519641202,\n",
       " 0.13138135294073283,\n",
       " 0.20290831670381876,\n",
       " 0.2533240430697724,\n",
       " 0.12251801429973153,\n",
       " 0.08043750795248408,\n",
       " 0.08411356804303632,\n",
       " 0.08673885385594943,\n",
       " 0.5903892558539414,\n",
       " 0.627552037887714,\n",
       " 0.28089945249906556,\n",
       " 0.13240722313679865,\n",
       " 0.15675645096470964,\n",
       " 0.13922782594036334,\n",
       " 0.21827903899573559,\n",
       " 0.18989383343454777,\n",
       " 0.12219206572010397,\n",
       " 0.10940869632766931,\n",
       " 0.16496278190268096,\n",
       " 0.09538166608290505,\n",
       " 0.1042312595365093,\n",
       " 0.08656080776415165,\n",
       " 0.24168363719135777,\n",
       " 0.16396192488162467,\n",
       " 0.3539971907394465,\n",
       " 0.5104799823377483,\n",
       " 0.2034759987615497,\n",
       " 0.21106399225965655,\n",
       " 0.5513865019506188,\n",
       " 0.8118492647886743,\n",
       " 0.466900832598197,\n",
       " 0.2720851315914678,\n",
       " 0.23746429508003059,\n",
       " 0.21485746802983324,\n",
       " 0.14048078162291194,\n",
       " 0.1630431325799552,\n",
       " 0.11395961670945465,\n",
       " 0.21124291846353993,\n",
       " 0.27657701292397996,\n",
       " 0.14657221335216997,\n",
       " 0.17479362500151535,\n",
       " 0.23028175115349403,\n",
       " 0.1486062801867036,\n",
       " 0.25199685669679345,\n",
       " 0.2687450811342061,\n",
       " 0.22495616556734777,\n",
       " 0.1769761535527942,\n",
       " 0.160106018598236,\n",
       " 0.08983589473748507,\n",
       " 0.1611270094650881,\n",
       " 0.15242674494271535,\n",
       " 0.1616863522454803,\n",
       " 0.12297745731203333,\n",
       " 0.10880388965045609,\n",
       " 0.10518714787511987,\n",
       " 0.06290769079195302,\n",
       " 0.08167259001838759,\n",
       " 0.24550714498312037,\n",
       " 0.36159447873822653,\n",
       " 0.30748419643372893,\n",
       " 0.40398761083759666,\n",
       " 0.14353054972376109,\n",
       " 0.17311398965085725,\n",
       " 0.175722962106449,\n",
       " 0.13850699483947665,\n",
       " 0.23920071747416305,\n",
       " 0.23966791873929888,\n",
       " 0.1353564255379806,\n",
       " 0.11009860775188055,\n",
       " 0.2560749205416194,\n",
       " 0.22137865903788218,\n",
       " 0.1746409291314563,\n",
       " 0.12754170074696236,\n",
       " 0.11810813974736839,\n",
       " 0.11804204258164336,\n",
       " 0.08996729272742499,\n",
       " 0.06317995468358201,\n",
       " 0.16201173470820873,\n",
       " 0.1206420341252413,\n",
       " 0.08234087609107837,\n",
       " 0.24696552653634213,\n",
       " 0.2568521993634084,\n",
       " 0.12043052013484787,\n",
       " 0.1334118864264608,\n",
       " 0.1094147698891749,\n",
       " 0.15921030363300595,\n",
       " 0.12695661175993453,\n",
       " 0.11568991256293878,\n",
       " 0.07935493865309495,\n",
       " 0.0751945001627971,\n",
       " 0.03860519172142384,\n",
       " 0.11075089240461466,\n",
       " 0.051809471986696866,\n",
       " 0.054591843358321575,\n",
       " 0.2739075221490698,\n",
       " 0.18749470786924652,\n",
       " 0.06536750259135096,\n",
       " 0.07541170736970634,\n",
       " 0.10626563754870993,\n",
       " 0.08062700401697198,\n",
       " 0.08085842792092054,\n",
       " 0.09046565114225949,\n",
       " 0.1374946384290379,\n",
       " 0.09588078159937208,\n",
       " 0.055445340798692615,\n",
       " 0.05814548696852124,\n",
       " 0.1777830640861839,\n",
       " 0.2834224398124248,\n",
       " 0.12677053870003246,\n",
       " 0.09484028444852649,\n",
       " 0.06092494485955131,\n",
       " 0.05239494594133228,\n",
       " 0.06310725084954727,\n",
       " 0.0840895272863455,\n",
       " 0.06297880708637627,\n",
       " 0.12040484571013138,\n",
       " 0.08341492554737326,\n",
       " 0.06701142914362174,\n",
       " 0.05163970406624297,\n",
       " 0.07448403457070155,\n",
       " 0.0685681465604735,\n",
       " 0.06065165063744827,\n",
       " 0.05150961816332985,\n",
       " 0.14426863621614686,\n",
       " 0.08556867209927471,\n",
       " 0.042211429030470524,\n",
       " 0.11676414887672498,\n",
       " 0.08061871211494087,\n",
       " 0.08323568113138491,\n",
       " 0.05749173529077683,\n",
       " 0.05497488593899725,\n",
       " 0.0172037422333274,\n",
       " 0.021501018611357677,\n",
       " 0.029271352995534065,\n",
       " 0.04572410142426404,\n",
       " 0.13150200760642478,\n",
       " 0.08738379593784906,\n",
       " 0.05985562800608519,\n",
       " 0.02172841780424132,\n",
       " 0.022813869633624022,\n",
       " 0.05439107468455511,\n",
       " 0.077484436854907,\n",
       " 0.036994538914591195,\n",
       " 0.03400466364418269,\n",
       " 0.039217043565703824,\n",
       " 0.021476068730745243,\n",
       " 0.02291035247179564,\n",
       " 0.03266049547477673,\n",
       " 0.03146653647664583,\n",
       " 0.03013241170042042,\n",
       " 0.02614756856736341,\n",
       " 0.016556819255411956,\n",
       " 0.0037980151511052655,\n",
       " 0.13039324658293203,\n",
       " 0.08918256043198436,\n",
       " 0.07710424944927749,\n",
       " 0.13767940093847061,\n",
       " 0.11415748068823098,\n",
       " 0.04544377998468501,\n",
       " 0.01995815598929649,\n",
       " 0.011515909292187466,\n",
       " 0.05250748391471669,\n",
       " 0.03910225636012614,\n",
       " 0.0382214587389411,\n",
       " 0.01961268254966167,\n",
       " 0.14003064674405302,\n",
       " 0.03508307461014688,\n",
       " 0.09687437895880607,\n",
       " 0.06612249783911262,\n",
       " 0.03824914139667137,\n",
       " 0.04296215963602442,\n",
       " 0.05498109218405698,\n",
       " 0.08217536826591354,\n",
       " 0.037819883936687816,\n",
       " 0.01684387572708472,\n",
       " 0.05734641498226609,\n",
       " 0.040660205442703434,\n",
       " 0.0429941179677951,\n",
       " 0.041875278838712525,\n",
       " 0.028927379935547858,\n",
       " 0.020063437901317135,\n",
       " 0.010883172047923318,\n",
       " 0.03029067239954253,\n",
       " 0.058786504816440856,\n",
       " 0.028987712481537732,\n",
       " 0.06607542204413033,\n",
       " 0.03636580564668238,\n",
       " 0.008168514458280467,\n",
       " 0.014887793777648088,\n",
       " 0.07953072612092116,\n",
       " 0.12708491337176675,\n",
       " 0.12784919306937254,\n",
       " 0.07291297556612528,\n",
       " 0.07215212364400822,\n",
       " 0.026320937330144872,\n",
       " 0.030939158514748474,\n",
       " 0.09575455983075654,\n",
       " 0.16573984145535475,\n",
       " 0.11956576655398726,\n",
       " 0.09048084446738461,\n",
       " 0.09210240486103255,\n",
       " 0.018577316966252734,\n",
       " 0.018112937815954064,\n",
       " 0.07867575520384085,\n",
       " 0.0979410118800649,\n",
       " 0.09079829218150487,\n",
       " 0.04488319500433947,\n",
       " 0.08145626819905746,\n",
       " 0.07192931432312769,\n",
       " 0.03808370756716063,\n",
       " 0.06202749162780787,\n",
       " 0.08354873203393345,\n",
       " 0.17772737518424223,\n",
       " 0.10755141291054598,\n",
       " 0.062438372718459444,\n",
       " 0.07254520641894024,\n",
       " 0.054180050437418116,\n",
       " 0.07556336294922839,\n",
       " 0.14169793751866494,\n",
       " 0.14361731478994566,\n",
       " 0.064521293879478,\n",
       " 0.0781088583932861,\n",
       " 0.04159427927628225,\n",
       " 0.024303555449170428,\n",
       " 0.05249744266783978,\n",
       " 0.04208020872236022,\n",
       " 0.13865619760382655,\n",
       " 0.043925017889302455,\n",
       " 0.048205749905920366,\n",
       " 0.025174105298592583,\n",
       " 0.02532914063868804,\n",
       " 0.04596960232099712,\n",
       " 0.09185665517614028,\n",
       " 0.052020642873405847,\n",
       " 0.05819561718485041,\n",
       " 0.05192157273498533,\n",
       " 0.03926376367035954,\n",
       " 0.035871894654268155,\n",
       " 0.038303214845378575,\n",
       " 0.05228403735146741,\n",
       " 0.13551408909081514,\n",
       " 0.15407778717841364,\n",
       " 0.057674160257422384,\n",
       " 0.051330385617497774,\n",
       " 0.03823614347061766,\n",
       " 0.04493401884729228,\n",
       " 0.05130166036352073,\n",
       " 0.049788041443311955,\n",
       " 0.07058136642762494,\n",
       " 0.05202640194267883,\n",
       " 0.012015731638266865,\n",
       " 0.012573576465923414,\n",
       " 0.06745378839461946,\n",
       " 0.03807411253057912,\n",
       " 0.0749666837482621,\n",
       " 0.041390439933745464,\n",
       " 0.08667858711466406,\n",
       " 0.02923047165360111,\n",
       " 0.013243832256583661,\n",
       " 0.032832739495294895,\n",
       " 0.06480245990494557,\n",
       " 0.035893936818083956,\n",
       " 0.052885745840871086,\n",
       " 0.06038327672972232,\n",
       " 0.018253271978696806,\n",
       " 0.01523083950336989,\n",
       " 0.045103770862001825,\n",
       " 0.02938361299577895,\n",
       " 0.03719826195880686,\n",
       " 0.03599313644470903,\n",
       " 0.025440205758611972,\n",
       " 0.0048869188663341855,\n",
       " 0.0028472047209107468,\n",
       " 0.03009424603085989,\n",
       " 0.02383651163320153,\n",
       " 0.03961446744830751,\n",
       " 0.12792725878822978,\n",
       " 0.03163926961216849,\n",
       " 0.01473803684376062,\n",
       " 0.013693041992358289,\n",
       " 0.15605987762101386,\n",
       " 0.06618704477428902,\n",
       " 0.031793266676105714,\n",
       " 0.041128920417715015,\n",
       " 0.015342792973031733,\n",
       " 0.01032826502201013,\n",
       " 0.004297865591897845,\n",
       " 0.017869821797127206,\n",
       " 0.01899137634402643,\n",
       " 0.027986099840047074,\n",
       " 0.0153368197660591,\n",
       " 0.017644257424207624,\n",
       " 0.0018466380720418944,\n",
       " 0.0,\n",
       " 0.03815707180486715,\n",
       " 0.019347937180659795,\n",
       " 0.028300955593731796,\n",
       " 0.022304275891561066,\n",
       " 0.019824863415599553,\n",
       " 0.003421110007086642,\n",
       " 0.01578579248280331,\n",
       " 0.019664940808072975,\n",
       " 0.031927205176587065,\n",
       " 0.035700874607078076,\n",
       " 0.030820495650930056,\n",
       " 0.024207538479237845,\n",
       " 0.002087302686283564,\n",
       " 0.0022814494927204353,\n",
       " 0.017011603474446774,\n",
       " 0.02413745348220949,\n",
       " 0.23819955932057335,\n",
       " 0.1699604847434836,\n",
       " 0.06403161468971294,\n",
       " 0.02413371250355481,\n",
       " 0.023678716843460484,\n",
       " 0.3616632306611043,\n",
       " 0.5377217521893534,\n",
       " 0.21785631519804513,\n",
       " 0.09288037418049533,\n",
       " 0.15131149433022162,\n",
       " 0.193838546746269,\n",
       " 0.3607873390313333,\n",
       " 0.3145086107028237,\n",
       " 0.18817665487156396,\n",
       " 0.2882244836682279,\n",
       " 0.16128762076519412,\n",
       " 0.15879142013557845,\n",
       " 0.06735716072165202,\n",
       " 0.07336227182447708,\n",
       " 0.12477313321934125,\n",
       " 0.13165671199225165,\n",
       " 0.11294468218411702,\n",
       " 0.22365411842340577,\n",
       " 0.30727084698995644,\n",
       " 0.10595269648444508,\n",
       " 0.08604129929760582,\n",
       " 0.09929601595142515,\n",
       " 0.08347330046253931,\n",
       " 0.07834783186879132,\n",
       " 0.1043383377069888,\n",
       " 0.12052719201057019,\n",
       " 0.06976144413598834,\n",
       " 0.04698958820112168,\n",
       " 0.18052773143210585,\n",
       " 0.13086571580495368,\n",
       " 0.24080298203841766,\n",
       " 0.26119276626272914,\n",
       " 0.18009912247871193,\n",
       " 0.0680006509820093,\n",
       " 0.07095207845102125,\n",
       " 0.16447935086304083,\n",
       " 0.11160092546970671,\n",
       " 0.06241197035477312,\n",
       " 0.09872060358657281,\n",
       " 0.1381619236060276,\n",
       " 0.07097019731839072,\n",
       " 0.057910989329359404,\n",
       " 0.09868956734788495,\n",
       " 0.07695701070518296,\n",
       " 0.07492543345295993,\n",
       " 0.06469218885148012,\n",
       " 0.06460334251250857,\n",
       " 0.040891895457577467,\n",
       " 0.07307708733703064,\n",
       " 0.07540070388537715,\n",
       " 0.10958851074984377,\n",
       " 0.07926475752185194,\n",
       " 0.19010874581482812,\n",
       " 0.1118492141927569,\n",
       " 0.033745061535021374,\n",
       " 0.05758763916766622,\n",
       " 0.0885308868952169,\n",
       " 0.08315583146178482,\n",
       " 0.06531830082878298,\n",
       " 0.06459267431127325,\n",
       " 0.03827390850986691,\n",
       " 0.03694786674606404,\n",
       " 0.051187084670405926,\n",
       " 0.04609408569120931,\n",
       " 0.0717385933823266,\n",
       " 0.061448650472728135,\n",
       " 0.039352344248590854,\n",
       " 0.04756689857390558,\n",
       " 0.03319165634207691,\n",
       " 0.024633090763403427,\n",
       " 0.12359032919925397,\n",
       " 0.07142491680781826,\n",
       " 0.06442485340981782,\n",
       " 0.05606120357847392,\n",
       " 0.06585175183083877,\n",
       " 0.020510365411802962,\n",
       " 0.02414104827329972,\n",
       " 0.04030450089841934,\n",
       " 0.04526478395059564,\n",
       " 0.07481436981772896,\n",
       " 0.061724112837282845,\n",
       " 0.1480086654930895,\n",
       " 0.029792148617815428,\n",
       " 0.030868002612550828,\n",
       " 0.06369204023177893,\n",
       " 0.060913912710950945,\n",
       " 0.04962359816883446,\n",
       " 0.04293264786875454,\n",
       " 0.053133413223248445,\n",
       " 0.02134325516179713,\n",
       " 0.040020086693672775,\n",
       " 0.14322007264668168,\n",
       " 0.11929323162842867,\n",
       " 0.06713376492786244,\n",
       " 0.06679483439964892,\n",
       " 0.05594258693281965,\n",
       " 0.08392420816601892,\n",
       " 0.1378103894373595,\n",
       " 0.08646077390843157,\n",
       " 0.04661825665930094,\n",
       " 0.06618437747615408,\n",
       " 0.07663040708395197,\n",
       " 0.046649365904800545,\n",
       " 0.015486432419298507,\n",
       " 0.020375265581421907,\n",
       " 0.06250294860525737,\n",
       " 0.0755045699270589,\n",
       " 0.04804810866605103,\n",
       " 0.055345741935189936,\n",
       " 0.06832429146434285,\n",
       " 0.037164408784432695,\n",
       " 0.016809351448972415,\n",
       " 0.04440073100228554,\n",
       " 0.04970249439794339,\n",
       " 0.038204690906589116,\n",
       " 0.05075922298733052,\n",
       " 0.04601575519728097,\n",
       " 0.04471637350301102,\n",
       " 0.014674020884661472,\n",
       " 0.03848246809081388,\n",
       " 0.03990662018051084,\n",
       " 0.05113023951755894,\n",
       " 0.07009935814685983,\n",
       " 0.03152704060755822,\n",
       " 0.022765737836308392,\n",
       " 0.00699193712337624,\n",
       " 0.044066566142670315,\n",
       " 0.026561072501619126,\n",
       " 0.051542085227462835,\n",
       " 0.030931081314176267,\n",
       " 0.07592658360654264,\n",
       " 0.03015063151602447,\n",
       " 0.01296318020116543,\n",
       " 0.05967493327508925,\n",
       " 0.3407541058744515,\n",
       " 0.34686585188635155,\n",
       " 0.1384438796768202,\n",
       " 0.0920200011659206,\n",
       " 0.09027258524203913,\n",
       " 0.07293907397029839,\n",
       " 0.1298922413575806,\n",
       " 0.07342624424227495,\n",
       " 0.1363725629174424,\n",
       " 0.13415367633881414,\n",
       " 0.07367834560620534,\n",
       " 0.019932654280436363,\n",
       " 0.021870814999008885,\n",
       " 0.0776550505558811,\n",
       " 0.0623349747576404,\n",
       " 0.05255794723312806,\n",
       " 0.05975579768354513,\n",
       " 0.03889240792096815,\n",
       " 0.026214289354215006,\n",
       " 0.03172195859181553,\n",
       " 0.0886173161324183,\n",
       " 0.14767994197084564,\n",
       " 0.11987753514565103,\n",
       " 0.14584745608346575,\n",
       " 0.11214302777597945,\n",
       " 0.0226965653749057,\n",
       " 0.026614154654018722,\n",
       " 0.05090202208789663,\n",
       " 0.04976291832120818,\n",
       " 0.046902593707275765,\n",
       " 0.05131098551990141,\n",
       " 0.16852549218822735,\n",
       " 0.08434777117522085,\n",
       " 0.03377424471226819,\n",
       " 0.06363966969308152,\n",
       " 0.13090584371977557,\n",
       " 0.09816460196521487,\n",
       " 0.12789391490258806,\n",
       " 0.14627900278641753,\n",
       " 0.32581713853045224,\n",
       " 0.3667602988347247,\n",
       " 0.4193399809478229,\n",
       " 0.3524390266318978,\n",
       " 0.2136343711462862,\n",
       " 0.2496058591811866,\n",
       " 0.4200464005829715,\n",
       " 0.08280085621567174,\n",
       " 0.21102057277722633,\n",
       " 0.21407353003539406,\n",
       " 0.11017854818400646,\n",
       " 0.1418902547894474,\n",
       " 0.118327479491899,\n",
       " 0.11810107583169727,\n",
       " 0.030909878825078202,\n",
       " 0.16870683235158832,\n",
       " 0.15414209025581227,\n",
       " 0.09479140246853465,\n",
       " 0.09763871582626527,\n",
       " 0.27226908591870524,\n",
       " 0.13165405482947082,\n",
       " 0.04765723047021909,\n",
       " 0.03653846885105017,\n",
       " 0.13341801701254813,\n",
       " 0.26614183238578326,\n",
       " 0.10579920805867274,\n",
       " 0.09497662493436002,\n",
       " 0.09684862804560113,\n",
       " 0.026685356711635216,\n",
       " 0.05030943812030959,\n",
       " 0.07336749048142704,\n",
       " 0.05484055821610726,\n",
       " 0.09974317899680499,\n",
       " 0.06321956585165481,\n",
       " 0.09936060802640387,\n",
       " 0.07558510042115238,\n",
       " 0.18887315796489745,\n",
       " 0.1703574864086774,\n",
       " 0.1322402176642876,\n",
       " 0.0665319297342617,\n",
       " 0.09411389678998389,\n",
       " 0.20571276528279578,\n",
       " 0.34195441204814037,\n",
       " 0.14255860196583206,\n",
       " 0.12755041301675374,\n",
       " 0.22862832526129298,\n",
       " 0.7497707808624061,\n",
       " 0.7040401708020491,\n",
       " 0.3674550592329966,\n",
       " 0.2352615740875837,\n",
       " 0.23916117234716594,\n",
       " 0.34617174398720796,\n",
       " 0.38443058147314024,\n",
       " 0.33296913907088754,\n",
       " 0.18922938318294458,\n",
       " 0.1715664446157848,\n",
       " 0.12108949295308563,\n",
       " 0.08325383313695967,\n",
       " 0.17263094558470568,\n",
       " 0.23741992454041666,\n",
       " 0.36365569541850673,\n",
       " 0.3213410211247035,\n",
       " 0.14841497385937202,\n",
       " 0.14610347939583695]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "n_train = int(len(X) - 7)\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0715 01:14:19.403719  7888 deprecation_wrapper.py:119] From C:\\Users\\ryan_\\.conda\\envs\\forecasting_tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0715 01:14:19.419718  7888 deprecation_wrapper.py:119] From C:\\Users\\ryan_\\.conda\\envs\\forecasting_tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0715 01:14:19.422712  7888 deprecation_wrapper.py:119] From C:\\Users\\ryan_\\.conda\\envs\\forecasting_tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0715 01:14:19.638643  7888 deprecation_wrapper.py:119] From C:\\Users\\ryan_\\.conda\\envs\\forecasting_tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0715 01:14:19.645641  7888 deprecation.py:506] From C:\\Users\\ryan_\\.conda\\envs\\forecasting_tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0715 01:14:20.407397  7888 deprecation_wrapper.py:119] From C:\\Users\\ryan_\\.conda\\envs\\forecasting_tf\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0715 01:14:20.740303  7888 deprecation.py:323] From C:\\Users\\ryan_\\.conda\\envs\\forecasting_tf\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0715 01:14:23.270499  7888 deprecation_wrapper.py:119] From C:\\Users\\ryan_\\.conda\\envs\\forecasting_tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " - 3s - loss: 0.0164 - mean_squared_error: 0.0164\n",
      "Epoch 2/500\n",
      " - 0s - loss: 0.0116 - mean_squared_error: 0.0116\n",
      "Epoch 3/500\n",
      " - 0s - loss: 0.0113 - mean_squared_error: 0.0113\n",
      "Epoch 4/500\n",
      " - 0s - loss: 0.0114 - mean_squared_error: 0.0114\n",
      "Epoch 5/500\n",
      " - 0s - loss: 0.0108 - mean_squared_error: 0.0108\n",
      "Epoch 6/500\n",
      " - 0s - loss: 0.0110 - mean_squared_error: 0.0110\n",
      "Epoch 7/500\n",
      " - 0s - loss: 0.0111 - mean_squared_error: 0.0111\n",
      "Epoch 8/500\n",
      " - 0s - loss: 0.0109 - mean_squared_error: 0.0109\n",
      "Epoch 9/500\n",
      " - 0s - loss: 0.0105 - mean_squared_error: 0.0105\n",
      "Epoch 10/500\n",
      " - 0s - loss: 0.0111 - mean_squared_error: 0.0111\n",
      "Epoch 11/500\n",
      " - 0s - loss: 0.0106 - mean_squared_error: 0.0106\n",
      "Epoch 12/500\n",
      " - 0s - loss: 0.0105 - mean_squared_error: 0.0105\n",
      "Epoch 13/500\n",
      " - 0s - loss: 0.0106 - mean_squared_error: 0.0106\n",
      "Epoch 14/500\n",
      " - 0s - loss: 0.0107 - mean_squared_error: 0.0107\n",
      "Epoch 15/500\n",
      " - 0s - loss: 0.0101 - mean_squared_error: 0.0101\n",
      "Epoch 16/500\n",
      " - 0s - loss: 0.0107 - mean_squared_error: 0.0107\n",
      "Epoch 17/500\n",
      " - 0s - loss: 0.0105 - mean_squared_error: 0.0105\n",
      "Epoch 18/500\n",
      " - 0s - loss: 0.0103 - mean_squared_error: 0.0103\n",
      "Epoch 19/500\n",
      " - 0s - loss: 0.0105 - mean_squared_error: 0.0105\n",
      "Epoch 20/500\n",
      " - 0s - loss: 0.0098 - mean_squared_error: 0.0098\n",
      "Epoch 21/500\n",
      " - 0s - loss: 0.0100 - mean_squared_error: 0.0100\n",
      "Epoch 22/500\n",
      " - 0s - loss: 0.0097 - mean_squared_error: 0.0097\n",
      "Epoch 23/500\n",
      " - 0s - loss: 0.0103 - mean_squared_error: 0.0103\n",
      "Epoch 24/500\n",
      " - 0s - loss: 0.0101 - mean_squared_error: 0.0101\n",
      "Epoch 25/500\n",
      " - 0s - loss: 0.0099 - mean_squared_error: 0.0099\n",
      "Epoch 26/500\n",
      " - 0s - loss: 0.0097 - mean_squared_error: 0.0097\n",
      "Epoch 27/500\n",
      " - 0s - loss: 0.0099 - mean_squared_error: 0.0099\n",
      "Epoch 28/500\n",
      " - 0s - loss: 0.0099 - mean_squared_error: 0.0099\n",
      "Epoch 29/500\n",
      " - 0s - loss: 0.0101 - mean_squared_error: 0.0101\n",
      "Epoch 30/500\n",
      " - 0s - loss: 0.0099 - mean_squared_error: 0.0099\n",
      "Epoch 31/500\n",
      " - 0s - loss: 0.0096 - mean_squared_error: 0.0096\n",
      "Epoch 32/500\n",
      " - 0s - loss: 0.0096 - mean_squared_error: 0.0096\n",
      "Epoch 33/500\n",
      " - 0s - loss: 0.0097 - mean_squared_error: 0.0097\n",
      "Epoch 34/500\n",
      " - 0s - loss: 0.0091 - mean_squared_error: 0.0091\n",
      "Epoch 35/500\n",
      " - 0s - loss: 0.0096 - mean_squared_error: 0.0096\n",
      "Epoch 36/500\n",
      " - 0s - loss: 0.0094 - mean_squared_error: 0.0094\n",
      "Epoch 37/500\n",
      " - 0s - loss: 0.0093 - mean_squared_error: 0.0093\n",
      "Epoch 38/500\n",
      " - 0s - loss: 0.0093 - mean_squared_error: 0.0093\n",
      "Epoch 39/500\n",
      " - 0s - loss: 0.0093 - mean_squared_error: 0.0093\n",
      "Epoch 40/500\n",
      " - 0s - loss: 0.0094 - mean_squared_error: 0.0094\n",
      "Epoch 41/500\n",
      " - 0s - loss: 0.0094 - mean_squared_error: 0.0094\n",
      "Epoch 42/500\n",
      " - 0s - loss: 0.0086 - mean_squared_error: 0.0086\n",
      "Epoch 43/500\n",
      " - 0s - loss: 0.0093 - mean_squared_error: 0.0093\n",
      "Epoch 44/500\n",
      " - 0s - loss: 0.0094 - mean_squared_error: 0.0094\n",
      "Epoch 45/500\n",
      " - 0s - loss: 0.0092 - mean_squared_error: 0.0092\n",
      "Epoch 46/500\n",
      " - 0s - loss: 0.0091 - mean_squared_error: 0.0091\n",
      "Epoch 47/500\n",
      " - 0s - loss: 0.0089 - mean_squared_error: 0.0089\n",
      "Epoch 48/500\n",
      " - 0s - loss: 0.0080 - mean_squared_error: 0.0080\n",
      "Epoch 49/500\n",
      " - 0s - loss: 0.0089 - mean_squared_error: 0.0089\n",
      "Epoch 50/500\n",
      " - 0s - loss: 0.0087 - mean_squared_error: 0.0087\n",
      "Epoch 51/500\n",
      " - 0s - loss: 0.0092 - mean_squared_error: 0.0092\n",
      "Epoch 52/500\n",
      " - 0s - loss: 0.0090 - mean_squared_error: 0.0090\n",
      "Epoch 53/500\n",
      " - 0s - loss: 0.0088 - mean_squared_error: 0.0088\n",
      "Epoch 54/500\n",
      " - 0s - loss: 0.0089 - mean_squared_error: 0.0089\n",
      "Epoch 55/500\n",
      " - 0s - loss: 0.0088 - mean_squared_error: 0.0088\n",
      "Epoch 56/500\n",
      " - 0s - loss: 0.0082 - mean_squared_error: 0.0082\n",
      "Epoch 57/500\n",
      " - 0s - loss: 0.0082 - mean_squared_error: 0.0082\n",
      "Epoch 58/500\n",
      " - 0s - loss: 0.0082 - mean_squared_error: 0.0082\n",
      "Epoch 59/500\n",
      " - 0s - loss: 0.0084 - mean_squared_error: 0.0084\n",
      "Epoch 60/500\n",
      " - 0s - loss: 0.0078 - mean_squared_error: 0.0078\n",
      "Epoch 61/500\n",
      " - 0s - loss: 0.0077 - mean_squared_error: 0.0077\n",
      "Epoch 62/500\n",
      " - 0s - loss: 0.0081 - mean_squared_error: 0.0081\n",
      "Epoch 63/500\n",
      " - 0s - loss: 0.0076 - mean_squared_error: 0.0076\n",
      "Epoch 64/500\n",
      " - 0s - loss: 0.0078 - mean_squared_error: 0.0078\n",
      "Epoch 65/500\n",
      " - 0s - loss: 0.0083 - mean_squared_error: 0.0083\n",
      "Epoch 66/500\n",
      " - 0s - loss: 0.0078 - mean_squared_error: 0.0078\n",
      "Epoch 67/500\n",
      " - 0s - loss: 0.0080 - mean_squared_error: 0.0080\n",
      "Epoch 68/500\n",
      " - 0s - loss: 0.0085 - mean_squared_error: 0.0085\n",
      "Epoch 69/500\n",
      " - 0s - loss: 0.0078 - mean_squared_error: 0.0078\n",
      "Epoch 70/500\n",
      " - 0s - loss: 0.0080 - mean_squared_error: 0.0080\n",
      "Epoch 71/500\n",
      " - 0s - loss: 0.0073 - mean_squared_error: 0.0073\n",
      "Epoch 72/500\n",
      " - 0s - loss: 0.0075 - mean_squared_error: 0.0075\n",
      "Epoch 73/500\n",
      " - 0s - loss: 0.0075 - mean_squared_error: 0.0075\n",
      "Epoch 74/500\n",
      " - 0s - loss: 0.0072 - mean_squared_error: 0.0072\n",
      "Epoch 75/500\n",
      " - 0s - loss: 0.0075 - mean_squared_error: 0.0075\n",
      "Epoch 76/500\n",
      " - 0s - loss: 0.0074 - mean_squared_error: 0.0074\n",
      "Epoch 77/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 78/500\n",
      " - 0s - loss: 0.0072 - mean_squared_error: 0.0072\n",
      "Epoch 79/500\n",
      " - 0s - loss: 0.0069 - mean_squared_error: 0.0069\n",
      "Epoch 80/500\n",
      " - 0s - loss: 0.0076 - mean_squared_error: 0.0076\n",
      "Epoch 81/500\n",
      " - 0s - loss: 0.0073 - mean_squared_error: 0.0073\n",
      "Epoch 82/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 83/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 84/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 85/500\n",
      " - 0s - loss: 0.0078 - mean_squared_error: 0.0078\n",
      "Epoch 86/500\n",
      " - 0s - loss: 0.0075 - mean_squared_error: 0.0075\n",
      "Epoch 87/500\n",
      " - 0s - loss: 0.0075 - mean_squared_error: 0.0075\n",
      "Epoch 88/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 89/500\n",
      " - 0s - loss: 0.0074 - mean_squared_error: 0.0074\n",
      "Epoch 90/500\n",
      " - 0s - loss: 0.0076 - mean_squared_error: 0.0076\n",
      "Epoch 91/500\n",
      " - 0s - loss: 0.0073 - mean_squared_error: 0.0073\n",
      "Epoch 92/500\n",
      " - 0s - loss: 0.0073 - mean_squared_error: 0.0073\n",
      "Epoch 93/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 94/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 95/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 96/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 97/500\n",
      " - 0s - loss: 0.0073 - mean_squared_error: 0.0073\n",
      "Epoch 98/500\n",
      " - 0s - loss: 0.0074 - mean_squared_error: 0.0074\n",
      "Epoch 99/500\n",
      " - 0s - loss: 0.0072 - mean_squared_error: 0.0072\n",
      "Epoch 100/500\n",
      " - 0s - loss: 0.0069 - mean_squared_error: 0.0069\n",
      "Epoch 101/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 102/500\n",
      " - 0s - loss: 0.0077 - mean_squared_error: 0.0077\n",
      "Epoch 103/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 104/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 105/500\n",
      " - 0s - loss: 0.0066 - mean_squared_error: 0.0066\n",
      "Epoch 106/500\n",
      " - 0s - loss: 0.0072 - mean_squared_error: 0.0072\n",
      "Epoch 107/500\n",
      " - 0s - loss: 0.0072 - mean_squared_error: 0.0072\n",
      "Epoch 108/500\n",
      " - 0s - loss: 0.0074 - mean_squared_error: 0.0074\n",
      "Epoch 109/500\n",
      " - 0s - loss: 0.0072 - mean_squared_error: 0.0072\n",
      "Epoch 110/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 111/500\n",
      " - 0s - loss: 0.0073 - mean_squared_error: 0.0073\n",
      "Epoch 112/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 113/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 114/500\n",
      " - 0s - loss: 0.0074 - mean_squared_error: 0.0074\n",
      "Epoch 115/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 116/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 117/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 118/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 119/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 120/500\n",
      " - 0s - loss: 0.0076 - mean_squared_error: 0.0076\n",
      "Epoch 121/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 122/500\n",
      " - 0s - loss: 0.0074 - mean_squared_error: 0.0074\n",
      "Epoch 123/500\n",
      " - 0s - loss: 0.0072 - mean_squared_error: 0.0072\n",
      "Epoch 124/500\n",
      " - 0s - loss: 0.0073 - mean_squared_error: 0.0073\n",
      "Epoch 125/500\n",
      " - 0s - loss: 0.0067 - mean_squared_error: 0.0067\n",
      "Epoch 126/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 127/500\n",
      " - 0s - loss: 0.0073 - mean_squared_error: 0.0073\n",
      "Epoch 128/500\n",
      " - 0s - loss: 0.0069 - mean_squared_error: 0.0069\n",
      "Epoch 129/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 130/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/500\n",
      " - 0s - loss: 0.0069 - mean_squared_error: 0.0069\n",
      "Epoch 132/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 133/500\n",
      " - 0s - loss: 0.0074 - mean_squared_error: 0.0074\n",
      "Epoch 134/500\n",
      " - 0s - loss: 0.0074 - mean_squared_error: 0.0074\n",
      "Epoch 135/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 136/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 137/500\n",
      " - 0s - loss: 0.0074 - mean_squared_error: 0.0074\n",
      "Epoch 138/500\n",
      " - 0s - loss: 0.0072 - mean_squared_error: 0.0072\n",
      "Epoch 139/500\n",
      " - 0s - loss: 0.0073 - mean_squared_error: 0.0073\n",
      "Epoch 140/500\n",
      " - 0s - loss: 0.0073 - mean_squared_error: 0.0073\n",
      "Epoch 141/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 142/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 143/500\n",
      " - 0s - loss: 0.0072 - mean_squared_error: 0.0072\n",
      "Epoch 144/500\n",
      " - 0s - loss: 0.0072 - mean_squared_error: 0.0072\n",
      "Epoch 145/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 146/500\n",
      " - 0s - loss: 0.0072 - mean_squared_error: 0.0072\n",
      "Epoch 147/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 148/500\n",
      " - 0s - loss: 0.0069 - mean_squared_error: 0.0069\n",
      "Epoch 149/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 150/500\n",
      " - 0s - loss: 0.0069 - mean_squared_error: 0.0069\n",
      "Epoch 151/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 152/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 153/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 154/500\n",
      " - 0s - loss: 0.0073 - mean_squared_error: 0.0073\n",
      "Epoch 155/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 156/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 157/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 158/500\n",
      " - 0s - loss: 0.0073 - mean_squared_error: 0.0073\n",
      "Epoch 159/500\n",
      " - 0s - loss: 0.0072 - mean_squared_error: 0.0072\n",
      "Epoch 160/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 161/500\n",
      " - 0s - loss: 0.0072 - mean_squared_error: 0.0072\n",
      "Epoch 162/500\n",
      " - 0s - loss: 0.0069 - mean_squared_error: 0.0069\n",
      "Epoch 163/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 164/500\n",
      " - 0s - loss: 0.0069 - mean_squared_error: 0.0069\n",
      "Epoch 165/500\n",
      " - 0s - loss: 0.0069 - mean_squared_error: 0.0069\n",
      "Epoch 166/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 167/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 168/500\n",
      " - 0s - loss: 0.0072 - mean_squared_error: 0.0072\n",
      "Epoch 169/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 170/500\n",
      " - 0s - loss: 0.0069 - mean_squared_error: 0.0069\n",
      "Epoch 171/500\n",
      " - 0s - loss: 0.0067 - mean_squared_error: 0.0067\n",
      "Epoch 172/500\n",
      " - 0s - loss: 0.0073 - mean_squared_error: 0.0073\n",
      "Epoch 173/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 174/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 175/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 176/500\n",
      " - 0s - loss: 0.0073 - mean_squared_error: 0.0073\n",
      "Epoch 177/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 178/500\n",
      " - 0s - loss: 0.0069 - mean_squared_error: 0.0069\n",
      "Epoch 179/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 180/500\n",
      " - 0s - loss: 0.0073 - mean_squared_error: 0.0073\n",
      "Epoch 181/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 182/500\n",
      " - 0s - loss: 0.0067 - mean_squared_error: 0.0067\n",
      "Epoch 183/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 184/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 185/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 186/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 187/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 188/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 189/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 190/500\n",
      " - 0s - loss: 0.0069 - mean_squared_error: 0.0069\n",
      "Epoch 191/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 192/500\n",
      " - 0s - loss: 0.0069 - mean_squared_error: 0.0069\n",
      "Epoch 193/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 194/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 195/500\n",
      " - 0s - loss: 0.0069 - mean_squared_error: 0.0069\n",
      "Epoch 196/500\n",
      " - 0s - loss: 0.0069 - mean_squared_error: 0.0069\n",
      "Epoch 197/500\n",
      " - 0s - loss: 0.0069 - mean_squared_error: 0.0069\n",
      "Epoch 198/500\n",
      " - 0s - loss: 0.0072 - mean_squared_error: 0.0072\n",
      "Epoch 199/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 200/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 201/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 202/500\n",
      " - 0s - loss: 0.0066 - mean_squared_error: 0.0066\n",
      "Epoch 203/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 204/500\n",
      " - 0s - loss: 0.0073 - mean_squared_error: 0.0073\n",
      "Epoch 205/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 206/500\n",
      " - 0s - loss: 0.0069 - mean_squared_error: 0.0069\n",
      "Epoch 207/500\n",
      " - 0s - loss: 0.0069 - mean_squared_error: 0.0069\n",
      "Epoch 208/500\n",
      " - 0s - loss: 0.0065 - mean_squared_error: 0.0065\n",
      "Epoch 209/500\n",
      " - 0s - loss: 0.0066 - mean_squared_error: 0.0066\n",
      "Epoch 210/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 211/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 212/500\n",
      " - 0s - loss: 0.0074 - mean_squared_error: 0.0074\n",
      "Epoch 213/500\n",
      " - 0s - loss: 0.0073 - mean_squared_error: 0.0073\n",
      "Epoch 214/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 215/500\n",
      " - 0s - loss: 0.0069 - mean_squared_error: 0.0069\n",
      "Epoch 216/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 217/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 218/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 219/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 220/500\n",
      " - 0s - loss: 0.0066 - mean_squared_error: 0.0066\n",
      "Epoch 221/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 222/500\n",
      " - 0s - loss: 0.0075 - mean_squared_error: 0.0075\n",
      "Epoch 223/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 224/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 225/500\n",
      " - 0s - loss: 0.0069 - mean_squared_error: 0.0069\n",
      "Epoch 226/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 227/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 228/500\n",
      " - 0s - loss: 0.0066 - mean_squared_error: 0.0066\n",
      "Epoch 229/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 230/500\n",
      " - 0s - loss: 0.0073 - mean_squared_error: 0.0073\n",
      "Epoch 231/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 232/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 233/500\n",
      " - 0s - loss: 0.0066 - mean_squared_error: 0.0066\n",
      "Epoch 234/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 235/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 236/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 237/500\n",
      " - 0s - loss: 0.0069 - mean_squared_error: 0.0069\n",
      "Epoch 238/500\n",
      " - 0s - loss: 0.0065 - mean_squared_error: 0.0065\n",
      "Epoch 239/500\n",
      " - 0s - loss: 0.0067 - mean_squared_error: 0.0067\n",
      "Epoch 240/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 241/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 242/500\n",
      " - 0s - loss: 0.0067 - mean_squared_error: 0.0067\n",
      "Epoch 243/500\n",
      " - 0s - loss: 0.0080 - mean_squared_error: 0.0080\n",
      "Epoch 244/500\n",
      " - 0s - loss: 0.0076 - mean_squared_error: 0.0076\n",
      "Epoch 245/500\n",
      " - 0s - loss: 0.0073 - mean_squared_error: 0.0073\n",
      "Epoch 246/500\n",
      " - 0s - loss: 0.0072 - mean_squared_error: 0.0072\n",
      "Epoch 247/500\n",
      " - 0s - loss: 0.0074 - mean_squared_error: 0.0074\n",
      "Epoch 248/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 249/500\n",
      " - 0s - loss: 0.0072 - mean_squared_error: 0.0072\n",
      "Epoch 250/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 251/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 252/500\n",
      " - 0s - loss: 0.0065 - mean_squared_error: 0.0065\n",
      "Epoch 253/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 254/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 255/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 256/500\n",
      " - 0s - loss: 0.0067 - mean_squared_error: 0.0067\n",
      "Epoch 257/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 258/500\n",
      " - 0s - loss: 0.0067 - mean_squared_error: 0.0067\n",
      "Epoch 259/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 260/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 261/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 262/500\n",
      " - 0s - loss: 0.0067 - mean_squared_error: 0.0067\n",
      "Epoch 263/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 264/500\n",
      " - 0s - loss: 0.0066 - mean_squared_error: 0.0066\n",
      "Epoch 265/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 266/500\n",
      " - 0s - loss: 0.0065 - mean_squared_error: 0.0065\n",
      "Epoch 267/500\n",
      " - 0s - loss: 0.0066 - mean_squared_error: 0.0066\n",
      "Epoch 268/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 269/500\n",
      " - 0s - loss: 0.0065 - mean_squared_error: 0.0065\n",
      "Epoch 270/500\n",
      " - 0s - loss: 0.0064 - mean_squared_error: 0.0064\n",
      "Epoch 271/500\n",
      " - 0s - loss: 0.0067 - mean_squared_error: 0.0067\n",
      "Epoch 272/500\n",
      " - 0s - loss: 0.0066 - mean_squared_error: 0.0066\n",
      "Epoch 273/500\n",
      " - 0s - loss: 0.0066 - mean_squared_error: 0.0066\n",
      "Epoch 274/500\n",
      " - 0s - loss: 0.0064 - mean_squared_error: 0.0064\n",
      "Epoch 275/500\n",
      " - 0s - loss: 0.0066 - mean_squared_error: 0.0066\n",
      "Epoch 276/500\n",
      " - 0s - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 277/500\n",
      " - 0s - loss: 0.0066 - mean_squared_error: 0.0066\n",
      "Epoch 278/500\n",
      " - 0s - loss: 0.0065 - mean_squared_error: 0.0065\n",
      "Epoch 279/500\n",
      " - 0s - loss: 0.0065 - mean_squared_error: 0.0065\n",
      "Epoch 280/500\n",
      " - 0s - loss: 0.0063 - mean_squared_error: 0.0063\n",
      "Epoch 281/500\n",
      " - 0s - loss: 0.0065 - mean_squared_error: 0.0065\n",
      "Epoch 282/500\n",
      " - 0s - loss: 0.0066 - mean_squared_error: 0.0066\n",
      "Epoch 283/500\n",
      " - 0s - loss: 0.0064 - mean_squared_error: 0.0064\n",
      "Epoch 284/500\n",
      " - 0s - loss: 0.0063 - mean_squared_error: 0.0063\n",
      "Epoch 285/500\n",
      " - 0s - loss: 0.0065 - mean_squared_error: 0.0065\n",
      "Epoch 286/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 287/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 288/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 289/500\n",
      " - 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 290/500\n",
      " - 0s - loss: 0.0064 - mean_squared_error: 0.0064\n",
      "Epoch 291/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 292/500\n",
      " - 0s - loss: 0.0066 - mean_squared_error: 0.0066\n",
      "Epoch 293/500\n",
      " - 0s - loss: 0.0064 - mean_squared_error: 0.0064\n",
      "Epoch 294/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 295/500\n",
      " - 0s - loss: 0.0066 - mean_squared_error: 0.0066\n",
      "Epoch 296/500\n",
      " - 0s - loss: 0.0072 - mean_squared_error: 0.0072\n",
      "Epoch 297/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 298/500\n",
      " - 0s - loss: 0.0066 - mean_squared_error: 0.0066\n",
      "Epoch 299/500\n",
      " - 0s - loss: 0.0064 - mean_squared_error: 0.0064\n",
      "Epoch 300/500\n",
      " - 0s - loss: 0.0064 - mean_squared_error: 0.0064\n",
      "Epoch 301/500\n",
      " - 0s - loss: 0.0063 - mean_squared_error: 0.0063\n",
      "Epoch 302/500\n",
      " - 0s - loss: 0.0066 - mean_squared_error: 0.0066\n",
      "Epoch 303/500\n",
      " - 0s - loss: 0.0065 - mean_squared_error: 0.0065\n",
      "Epoch 304/500\n",
      " - 0s - loss: 0.0064 - mean_squared_error: 0.0064\n",
      "Epoch 305/500\n",
      " - 0s - loss: 0.0064 - mean_squared_error: 0.0064\n",
      "Epoch 306/500\n",
      " - 0s - loss: 0.0064 - mean_squared_error: 0.0064\n",
      "Epoch 307/500\n",
      " - 0s - loss: 0.0063 - mean_squared_error: 0.0063\n",
      "Epoch 308/500\n",
      " - 0s - loss: 0.0063 - mean_squared_error: 0.0063\n",
      "Epoch 309/500\n",
      " - 0s - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Epoch 310/500\n",
      " - 0s - loss: 0.0065 - mean_squared_error: 0.0065\n",
      "Epoch 311/500\n",
      " - 0s - loss: 0.0065 - mean_squared_error: 0.0065\n",
      "Epoch 312/500\n",
      " - 0s - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Epoch 313/500\n",
      " - 0s - loss: 0.0058 - mean_squared_error: 0.0058\n",
      "Epoch 314/500\n",
      " - 0s - loss: 0.0064 - mean_squared_error: 0.0064\n",
      "Epoch 315/500\n",
      " - 0s - loss: 0.0062 - mean_squared_error: 0.0062\n",
      "Epoch 316/500\n",
      " - 0s - loss: 0.0063 - mean_squared_error: 0.0063\n",
      "Epoch 317/500\n",
      " - 0s - loss: 0.0064 - mean_squared_error: 0.0064\n",
      "Epoch 318/500\n",
      " - 0s - loss: 0.0063 - mean_squared_error: 0.0063\n",
      "Epoch 319/500\n",
      " - 0s - loss: 0.0063 - mean_squared_error: 0.0063\n",
      "Epoch 320/500\n",
      " - 0s - loss: 0.0066 - mean_squared_error: 0.0066\n",
      "Epoch 321/500\n",
      " - 0s - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 322/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 323/500\n",
      " - 0s - loss: 0.0062 - mean_squared_error: 0.0062\n",
      "Epoch 324/500\n",
      " - 0s - loss: 0.0062 - mean_squared_error: 0.0062\n",
      "Epoch 325/500\n",
      " - 0s - loss: 0.0064 - mean_squared_error: 0.0064\n",
      "Epoch 326/500\n",
      " - 0s - loss: 0.0062 - mean_squared_error: 0.0062\n",
      "Epoch 327/500\n",
      " - 0s - loss: 0.0065 - mean_squared_error: 0.0065\n",
      "Epoch 328/500\n",
      " - 0s - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Epoch 329/500\n",
      " - 0s - loss: 0.0063 - mean_squared_error: 0.0063\n",
      "Epoch 330/500\n",
      " - 0s - loss: 0.0062 - mean_squared_error: 0.0062\n",
      "Epoch 331/500\n",
      " - 0s - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 332/500\n",
      " - 0s - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Epoch 333/500\n",
      " - 0s - loss: 0.0059 - mean_squared_error: 0.0059\n",
      "Epoch 334/500\n",
      " - 0s - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 335/500\n",
      " - 0s - loss: 0.0065 - mean_squared_error: 0.0065\n",
      "Epoch 336/500\n",
      " - 0s - loss: 0.0063 - mean_squared_error: 0.0063\n",
      "Epoch 337/500\n",
      " - 0s - loss: 0.0062 - mean_squared_error: 0.0062\n",
      "Epoch 338/500\n",
      " - 0s - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Epoch 339/500\n",
      " - 0s - loss: 0.0064 - mean_squared_error: 0.0064\n",
      "Epoch 340/500\n",
      " - 0s - loss: 0.0062 - mean_squared_error: 0.0062\n",
      "Epoch 341/500\n",
      " - 0s - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Epoch 342/500\n",
      " - 0s - loss: 0.0064 - mean_squared_error: 0.0064\n",
      "Epoch 343/500\n",
      " - 0s - loss: 0.0064 - mean_squared_error: 0.0064\n",
      "Epoch 344/500\n",
      " - 0s - loss: 0.0064 - mean_squared_error: 0.0064\n",
      "Epoch 345/500\n",
      " - 0s - loss: 0.0068 - mean_squared_error: 0.0068\n",
      "Epoch 346/500\n",
      " - 0s - loss: 0.0062 - mean_squared_error: 0.0062\n",
      "Epoch 347/500\n",
      " - 0s - loss: 0.0063 - mean_squared_error: 0.0063\n",
      "Epoch 348/500\n",
      " - 0s - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 349/500\n",
      " - 0s - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 350/500\n",
      " - 0s - loss: 0.0062 - mean_squared_error: 0.0062\n",
      "Epoch 351/500\n",
      " - 0s - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 352/500\n",
      " - 0s - loss: 0.0066 - mean_squared_error: 0.0066\n",
      "Epoch 353/500\n",
      " - 0s - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Epoch 354/500\n",
      " - 0s - loss: 0.0063 - mean_squared_error: 0.0063\n",
      "Epoch 355/500\n",
      " - 0s - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Epoch 356/500\n",
      " - 0s - loss: 0.0065 - mean_squared_error: 0.0065\n",
      "Epoch 357/500\n",
      " - 0s - loss: 0.0065 - mean_squared_error: 0.0065\n",
      "Epoch 358/500\n",
      " - 0s - loss: 0.0064 - mean_squared_error: 0.0064\n",
      "Epoch 359/500\n",
      " - 0s - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 360/500\n",
      " - 0s - loss: 0.0063 - mean_squared_error: 0.0063\n",
      "Epoch 361/500\n",
      " - 0s - loss: 0.0062 - mean_squared_error: 0.0062\n",
      "Epoch 362/500\n",
      " - 0s - loss: 0.0063 - mean_squared_error: 0.0063\n",
      "Epoch 363/500\n",
      " - 0s - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 364/500\n",
      " - 0s - loss: 0.0058 - mean_squared_error: 0.0058\n",
      "Epoch 365/500\n",
      " - 0s - loss: 0.0063 - mean_squared_error: 0.0063\n",
      "Epoch 366/500\n",
      " - 0s - loss: 0.0058 - mean_squared_error: 0.0058\n",
      "Epoch 367/500\n",
      " - 0s - loss: 0.0058 - mean_squared_error: 0.0058\n",
      "Epoch 368/500\n",
      " - 0s - loss: 0.0058 - mean_squared_error: 0.0058\n",
      "Epoch 369/500\n",
      " - 0s - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Epoch 370/500\n",
      " - 0s - loss: 0.0059 - mean_squared_error: 0.0059\n",
      "Epoch 371/500\n",
      " - 0s - loss: 0.0062 - mean_squared_error: 0.0062\n",
      "Epoch 372/500\n",
      " - 0s - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 373/500\n",
      " - 0s - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 374/500\n",
      " - 0s - loss: 0.0062 - mean_squared_error: 0.0062\n",
      "Epoch 375/500\n",
      " - 0s - loss: 0.0065 - mean_squared_error: 0.0065\n",
      "Epoch 376/500\n",
      " - 0s - loss: 0.0063 - mean_squared_error: 0.0063\n",
      "Epoch 377/500\n",
      " - 0s - loss: 0.0063 - mean_squared_error: 0.0063\n",
      "Epoch 378/500\n",
      " - 0s - loss: 0.0062 - mean_squared_error: 0.0062\n",
      "Epoch 379/500\n",
      " - 0s - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Epoch 380/500\n",
      " - 0s - loss: 0.0057 - mean_squared_error: 0.0057\n",
      "Epoch 381/500\n",
      " - 0s - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 382/500\n",
      " - 0s - loss: 0.0058 - mean_squared_error: 0.0058\n",
      "Epoch 383/500\n",
      " - 0s - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Epoch 384/500\n",
      " - 0s - loss: 0.0058 - mean_squared_error: 0.0058\n",
      "Epoch 385/500\n",
      " - 0s - loss: 0.0059 - mean_squared_error: 0.0059\n",
      "Epoch 386/500\n",
      " - 0s - loss: 0.0062 - mean_squared_error: 0.0062\n",
      "Epoch 387/500\n",
      " - 0s - loss: 0.0057 - mean_squared_error: 0.0057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 388/500\n",
      " - 0s - loss: 0.0063 - mean_squared_error: 0.0063\n",
      "Epoch 389/500\n",
      " - 0s - loss: 0.0059 - mean_squared_error: 0.0059\n",
      "Epoch 390/500\n",
      " - 0s - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Epoch 391/500\n",
      " - 0s - loss: 0.0057 - mean_squared_error: 0.0057\n",
      "Epoch 392/500\n",
      " - 0s - loss: 0.0056 - mean_squared_error: 0.0056\n",
      "Epoch 393/500\n",
      " - 0s - loss: 0.0056 - mean_squared_error: 0.0056\n",
      "Epoch 394/500\n",
      " - 0s - loss: 0.0054 - mean_squared_error: 0.0054\n",
      "Epoch 395/500\n",
      " - 0s - loss: 0.0054 - mean_squared_error: 0.0054\n",
      "Epoch 396/500\n",
      " - 0s - loss: 0.0052 - mean_squared_error: 0.0052\n",
      "Epoch 397/500\n",
      " - 0s - loss: 0.0056 - mean_squared_error: 0.0056\n",
      "Epoch 398/500\n",
      " - 0s - loss: 0.0054 - mean_squared_error: 0.0054\n",
      "Epoch 399/500\n",
      " - 0s - loss: 0.0054 - mean_squared_error: 0.0054\n",
      "Epoch 400/500\n",
      " - 0s - loss: 0.0058 - mean_squared_error: 0.0058\n",
      "Epoch 401/500\n",
      " - 0s - loss: 0.0057 - mean_squared_error: 0.0057\n",
      "Epoch 402/500\n",
      " - 0s - loss: 0.0052 - mean_squared_error: 0.0052\n",
      "Epoch 403/500\n",
      " - 0s - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 404/500\n",
      " - 0s - loss: 0.0054 - mean_squared_error: 0.0054\n",
      "Epoch 405/500\n",
      " - 0s - loss: 0.0056 - mean_squared_error: 0.0056\n",
      "Epoch 406/500\n",
      " - 0s - loss: 0.0059 - mean_squared_error: 0.0059\n",
      "Epoch 407/500\n",
      " - 0s - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 408/500\n",
      " - 0s - loss: 0.0055 - mean_squared_error: 0.0055\n",
      "Epoch 409/500\n",
      " - 0s - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 410/500\n",
      " - 0s - loss: 0.0059 - mean_squared_error: 0.0059\n",
      "Epoch 411/500\n",
      " - 0s - loss: 0.0063 - mean_squared_error: 0.0063\n",
      "Epoch 412/500\n",
      " - 0s - loss: 0.0065 - mean_squared_error: 0.0065\n",
      "Epoch 413/500\n",
      " - 0s - loss: 0.0066 - mean_squared_error: 0.0066\n",
      "Epoch 414/500\n",
      " - 0s - loss: 0.0064 - mean_squared_error: 0.0064\n",
      "Epoch 415/500\n",
      " - 0s - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Epoch 416/500\n",
      " - 0s - loss: 0.0055 - mean_squared_error: 0.0055\n",
      "Epoch 417/500\n",
      " - 0s - loss: 0.0059 - mean_squared_error: 0.0059\n",
      "Epoch 418/500\n",
      " - 0s - loss: 0.0056 - mean_squared_error: 0.0056\n",
      "Epoch 419/500\n",
      " - 0s - loss: 0.0058 - mean_squared_error: 0.0058\n",
      "Epoch 420/500\n",
      " - 0s - loss: 0.0055 - mean_squared_error: 0.0055\n",
      "Epoch 421/500\n",
      " - 0s - loss: 0.0059 - mean_squared_error: 0.0059\n",
      "Epoch 422/500\n",
      " - 0s - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Epoch 423/500\n",
      " - 0s - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 424/500\n",
      " - 0s - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Epoch 425/500\n",
      " - 0s - loss: 0.0058 - mean_squared_error: 0.0058\n",
      "Epoch 426/500\n",
      " - 0s - loss: 0.0056 - mean_squared_error: 0.0056\n",
      "Epoch 427/500\n",
      " - 0s - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 428/500\n",
      " - 0s - loss: 0.0055 - mean_squared_error: 0.0055\n",
      "Epoch 429/500\n",
      " - 0s - loss: 0.0056 - mean_squared_error: 0.0056\n",
      "Epoch 430/500\n",
      " - 0s - loss: 0.0056 - mean_squared_error: 0.0056\n",
      "Epoch 431/500\n",
      " - 0s - loss: 0.0055 - mean_squared_error: 0.0055\n",
      "Epoch 432/500\n",
      " - 0s - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 433/500\n",
      " - 0s - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 434/500\n",
      " - 0s - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 435/500\n",
      " - 0s - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 436/500\n",
      " - 0s - loss: 0.0058 - mean_squared_error: 0.0058\n",
      "Epoch 437/500\n",
      " - 0s - loss: 0.0059 - mean_squared_error: 0.0059\n",
      "Epoch 438/500\n",
      " - 0s - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 439/500\n",
      " - 0s - loss: 0.0054 - mean_squared_error: 0.0054\n",
      "Epoch 440/500\n",
      " - 0s - loss: 0.0055 - mean_squared_error: 0.0055\n",
      "Epoch 441/500\n",
      " - 0s - loss: 0.0054 - mean_squared_error: 0.0054\n",
      "Epoch 442/500\n",
      " - 0s - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 443/500\n",
      " - 0s - loss: 0.0055 - mean_squared_error: 0.0055\n",
      "Epoch 444/500\n",
      " - 0s - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 445/500\n",
      " - 0s - loss: 0.0054 - mean_squared_error: 0.0054\n",
      "Epoch 446/500\n",
      " - 0s - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 447/500\n",
      " - 0s - loss: 0.0055 - mean_squared_error: 0.0055\n",
      "Epoch 448/500\n",
      " - 0s - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 449/500\n",
      " - 0s - loss: 0.0062 - mean_squared_error: 0.0062\n",
      "Epoch 450/500\n",
      " - 0s - loss: 0.0061 - mean_squared_error: 0.0061\n",
      "Epoch 451/500\n",
      " - 0s - loss: 0.0058 - mean_squared_error: 0.0058\n",
      "Epoch 452/500\n",
      " - 0s - loss: 0.0056 - mean_squared_error: 0.0056\n",
      "Epoch 453/500\n",
      " - 0s - loss: 0.0058 - mean_squared_error: 0.0058\n",
      "Epoch 454/500\n",
      " - 0s - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "Epoch 455/500\n",
      " - 0s - loss: 0.0054 - mean_squared_error: 0.0054\n",
      "Epoch 456/500\n",
      " - 0s - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 457/500\n",
      " - 0s - loss: 0.0055 - mean_squared_error: 0.0055\n",
      "Epoch 458/500\n",
      " - 0s - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 459/500\n",
      " - 0s - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 460/500\n",
      " - 0s - loss: 0.0054 - mean_squared_error: 0.0054\n",
      "Epoch 461/500\n",
      " - 0s - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 462/500\n",
      " - 0s - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 463/500\n",
      " - 0s - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 464/500\n",
      " - 0s - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 465/500\n",
      " - 0s - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 466/500\n",
      " - 0s - loss: 0.0052 - mean_squared_error: 0.0052\n",
      "Epoch 467/500\n",
      " - 0s - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "Epoch 468/500\n",
      " - 0s - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 469/500\n",
      " - 0s - loss: 0.0056 - mean_squared_error: 0.0056\n",
      "Epoch 470/500\n",
      " - 0s - loss: 0.0054 - mean_squared_error: 0.0054\n",
      "Epoch 471/500\n",
      " - 0s - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "Epoch 472/500\n",
      " - 0s - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 473/500\n",
      " - 0s - loss: 0.0055 - mean_squared_error: 0.0055\n",
      "Epoch 474/500\n",
      " - 0s - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 475/500\n",
      " - 0s - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "Epoch 476/500\n",
      " - 0s - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 477/500\n",
      " - 0s - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "Epoch 478/500\n",
      " - 0s - loss: 0.0055 - mean_squared_error: 0.0055\n",
      "Epoch 479/500\n",
      " - 0s - loss: 0.0056 - mean_squared_error: 0.0056\n",
      "Epoch 480/500\n",
      " - 0s - loss: 0.0056 - mean_squared_error: 0.0056\n",
      "Epoch 481/500\n",
      " - 0s - loss: 0.0054 - mean_squared_error: 0.0054\n",
      "Epoch 482/500\n",
      " - 0s - loss: 0.0052 - mean_squared_error: 0.0052\n",
      "Epoch 483/500\n",
      " - 0s - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "Epoch 484/500\n",
      " - 0s - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 485/500\n",
      " - 0s - loss: 0.0049 - mean_squared_error: 0.0049\n",
      "Epoch 486/500\n",
      " - 0s - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 487/500\n",
      " - 0s - loss: 0.0049 - mean_squared_error: 0.0049\n",
      "Epoch 488/500\n",
      " - 0s - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 489/500\n",
      " - 0s - loss: 0.0049 - mean_squared_error: 0.0049\n",
      "Epoch 490/500\n",
      " - 0s - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 491/500\n",
      " - 0s - loss: 0.0049 - mean_squared_error: 0.0049\n",
      "Epoch 492/500\n",
      " - 0s - loss: 0.0049 - mean_squared_error: 0.0049\n",
      "Epoch 493/500\n",
      " - 0s - loss: 0.0048 - mean_squared_error: 0.0048\n",
      "Epoch 494/500\n",
      " - 0s - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "Epoch 495/500\n",
      " - 0s - loss: 0.0052 - mean_squared_error: 0.0052\n",
      "Epoch 496/500\n",
      " - 0s - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 497/500\n",
      " - 0s - loss: 0.0054 - mean_squared_error: 0.0054\n",
      "Epoch 498/500\n",
      " - 0s - loss: 0.0047 - mean_squared_error: 0.0047\n",
      "Epoch 499/500\n",
      " - 0s - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "Epoch 500/500\n",
      " - 0s - loss: 0.0051 - mean_squared_error: 0.0051\n"
     ]
    }
   ],
   "source": [
    "regressor = Sequential()\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (trainX.shape[1], 1)))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(Dense(units = 30))\n",
    "regressor.add(Dropout(0.3))\n",
    "regressor.add(Dense(units = 1))\n",
    "\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error',metrics=['mse'])\n",
    "\n",
    "'''\n",
    "es = EarlyStopping(monitor='val_loss',\n",
    "                                  min_delta=0,\n",
    "                                  patience=40,\n",
    "                                  verbose=0, mode='auto')\n",
    "'''\n",
    "\n",
    "history = regressor.fit(X, y, epochs =500, batch_size = 64, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.save(\"forecasting_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfjs.converters.save_keras_model(regressor, \"forecast_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame([predicted, testy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predicted=regressor.predict(X[-30:])\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(predicted.squeeze(), label=\"predicted\")\n",
    "plt.plot(y[-30:].squeeze(), label=\"actual\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_input = X[-1]\n",
    "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_=df.Avg_Max_Temp.values.tolist()[-44:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(test[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test=np.array(test_[:30]).reshape((1, 30, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_y = model.predict(test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(test_y[0]), label = \"predicted\")\n",
    "plt.plot(test_[30:], label='true_values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
